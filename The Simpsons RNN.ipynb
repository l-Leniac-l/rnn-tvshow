{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simpsons - Recurrent Neural Network\n",
    "\n",
    "In this project we'll generate Simpsons TV Scripts using RNN's.\n",
    "\n",
    "You can find the dataset at [kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).\n",
    "\n",
    "The dataset consists of 27 seasons of The Simpsons show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) and extract the csv files in `data` folder.\n",
    "\n",
    "Let's import our data using `pandas.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148761</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148762</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148763</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148764</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148765</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        episode_id  number                                       raw_text\n",
       "148761           1       0   (Street: ext. street - establishing - night)\n",
       "148762           1       1                        (Car: int. car - night)\n",
       "148763           1       2            Marge Simpson: Ooo, careful, Homer.\n",
       "148764           1       3  Homer Simpson: There's no time to be careful.\n",
       "148765           1       4                     Homer Simpson: We're late."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the csv file\n",
    "simpsons_lines = pd.read_csv('data/simpsons_script_lines.csv',\n",
    "                             usecols=['episode_id', 'number', 'raw_text'],\n",
    "                             error_bad_lines=False,\n",
    "                             low_memory=False)\n",
    "\n",
    "# Sorting the data\n",
    "simpsons_lines.sort_values(['episode_id', 'number'], axis=0, ascending=True, inplace=True)\n",
    "\n",
    "simpsons_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148761     (Street: ext. street - establishing - night)\n",
       "148762                          (Car: int. car - night)\n",
       "148763              Marge Simpson: Ooo, careful, Homer.\n",
       "148764    Homer Simpson: There's no time to be careful.\n",
       "148765                       Homer Simpson: We're late.\n",
       "Name: raw_text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets the raw text\n",
    "simpsons_lines_raw = simpsons_lines['raw_text']\n",
    "simpsons_lines_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some data info\n",
    "\n",
    "Let's print some info about your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 568\n",
      "Number of lines: 147786\n"
     ]
    }
   ],
   "source": [
    "print('Number of episodes: {0}'.format(simpsons_lines.tail(1)['episode_id'].item()))\n",
    "print('Number of lines: {0}'.format(simpsons_lines.tail(1).index.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "We'll preprocess our data to avoid unnecessary computations at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our lookup table will contain two dicts, with the following structure:\n",
    "```python\n",
    "vocab_to_int = {\n",
    "  'hello': 1\n",
    "}\n",
    "int_to_vocab = {\n",
    "  1: 'hello'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(vocab)}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize our punctuation. If we don't do this, the RNN will see `'word' != 'word!'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    tokens = {\n",
    "        '.'  : 'period',\n",
    "        ','  : 'comma',\n",
    "        '\"'  : 'quote',\n",
    "        ';'  : 'semicolon',\n",
    "        '!'  : 'exclamation_mark',\n",
    "        '?'  : 'question_mark',\n",
    "        '('  : 'parentheses_left',\n",
    "        ')'  : 'parentheses_right',\n",
    "        '--' : 'dash',\n",
    "        '\\n' : 'return'\n",
    "    }\n",
    "    return {token: '||{0}||'.format(value) for token, value in tokens.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our save function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save(dataset, token_lookup, create_lookup_tables):\n",
    "    text = ''\n",
    "    for idx, line in dataset.items():\n",
    "        text += line + '\\n'\n",
    "    \n",
    "    token_dict = token_lookup()\n",
    "    text = text.split()\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing and saving\n",
    "preprocess_and_save(simpsons_lines_raw, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our RNN.\n",
    "\n",
    "We'll use LSTM (Long short-term Memory) cells to build this.\n",
    "\n",
    "You can find more about LSTM [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading preprocessed data\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating input tensors\n",
    "def get_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, shape=(None, None), name='input')\n",
    "    targets = tf.placeholder(tf.int32, shape=(None, None), name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=(None), name='learning_rate')\n",
    "\n",
    "    return inputs, targets, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the initial cell with zero state\n",
    "def get_init_cell(lstm_cell_number, batch_size, rnn_size):\n",
    "    lstm_cells = [tf.contrib.rnn.BasicLSTMCell(rnn_size) for i in range(0, lstm_cell_number)]\n",
    "    \n",
    "    drop = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.5) for lstm in lstm_cells]\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "    \n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), name='initial_state')\n",
    "\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embed the data with random_uniform distribution\n",
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the rnn\n",
    "def build_rnn(cell, inputs):\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    return outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the entire NN\n",
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    \n",
    "    outputs, state = build_rnn(cell, embed)\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        outputs, num_outputs = vocab_size, activation_fn = None\n",
    "    )\n",
    "    \n",
    "    return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate batches to avoid OOM (out of memory)\n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "\n",
    "    # Drop the last few characters to make only full batches\n",
    "    xdata = np.array(int_text[: n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1: n_batches * batch_size * seq_length + 1])\n",
    "\n",
    "    #ydata[:-1] = xdata[:1]\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "    y_batches[-1][-1][-1] = x_batches[0][0][0]\n",
    "\n",
    "    return np.array(list(zip(x_batches, y_batches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Let's train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Defines hyperparameters\n",
    "\n",
    "# Number of Epochs\n",
    "num_epochs = 200\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 128\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 16\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 150\n",
    "# Number of lstm cells\n",
    "lstm_cell_number = 2\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the seq2seq model\n",
    "\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(lstm_cell_number, input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/450   train_loss = 11.825\n",
      "Epoch   0 Batch  150/450   train_loss = 8.348\n",
      "Epoch   0 Batch  300/450   train_loss = 8.311\n",
      "Epoch   1 Batch    0/450   train_loss = 8.388\n",
      "Epoch   1 Batch  150/450   train_loss = 8.192\n",
      "Epoch   1 Batch  300/450   train_loss = 8.203\n",
      "Epoch   2 Batch    0/450   train_loss = 8.319\n",
      "Epoch   2 Batch  150/450   train_loss = 8.186\n",
      "Epoch   2 Batch  300/450   train_loss = 8.171\n",
      "Epoch   3 Batch    0/450   train_loss = 8.264\n",
      "Epoch   3 Batch  150/450   train_loss = 8.145\n",
      "Epoch   3 Batch  300/450   train_loss = 8.100\n",
      "Epoch   4 Batch    0/450   train_loss = 7.940\n",
      "Epoch   4 Batch  150/450   train_loss = 7.613\n",
      "Epoch   4 Batch  300/450   train_loss = 7.353\n",
      "Epoch   5 Batch    0/450   train_loss = 7.097\n",
      "Epoch   5 Batch  150/450   train_loss = 6.802\n",
      "Epoch   5 Batch  300/450   train_loss = 6.545\n",
      "Epoch   6 Batch    0/450   train_loss = 6.449\n",
      "Epoch   6 Batch  150/450   train_loss = 6.335\n",
      "Epoch   6 Batch  300/450   train_loss = 6.144\n",
      "Epoch   7 Batch    0/450   train_loss = 6.112\n",
      "Epoch   7 Batch  150/450   train_loss = 6.068\n",
      "Epoch   7 Batch  300/450   train_loss = 5.896\n",
      "Epoch   8 Batch    0/450   train_loss = 5.888\n",
      "Epoch   8 Batch  150/450   train_loss = 5.866\n",
      "Epoch   8 Batch  300/450   train_loss = 5.708\n",
      "Epoch   9 Batch    0/450   train_loss = 5.702\n",
      "Epoch   9 Batch  150/450   train_loss = 5.702\n",
      "Epoch   9 Batch  300/450   train_loss = 5.551\n",
      "Epoch  10 Batch    0/450   train_loss = 5.537\n",
      "Epoch  10 Batch  150/450   train_loss = 5.555\n",
      "Epoch  10 Batch  300/450   train_loss = 5.418\n",
      "Epoch  11 Batch    0/450   train_loss = 5.387\n",
      "Epoch  11 Batch  150/450   train_loss = 5.420\n",
      "Epoch  11 Batch  300/450   train_loss = 5.288\n",
      "Epoch  12 Batch    0/450   train_loss = 5.249\n",
      "Epoch  12 Batch  150/450   train_loss = 5.300\n",
      "Epoch  12 Batch  300/450   train_loss = 5.186\n",
      "Epoch  13 Batch    0/450   train_loss = 5.128\n",
      "Epoch  13 Batch  150/450   train_loss = 5.187\n",
      "Epoch  13 Batch  300/450   train_loss = 5.070\n",
      "Epoch  14 Batch    0/450   train_loss = 5.008\n",
      "Epoch  14 Batch  150/450   train_loss = 5.079\n",
      "Epoch  14 Batch  300/450   train_loss = 4.961\n",
      "Epoch  15 Batch    0/450   train_loss = 4.902\n",
      "Epoch  15 Batch  150/450   train_loss = 4.977\n",
      "Epoch  15 Batch  300/450   train_loss = 4.851\n",
      "Epoch  16 Batch    0/450   train_loss = 4.803\n",
      "Epoch  16 Batch  150/450   train_loss = 4.876\n",
      "Epoch  16 Batch  300/450   train_loss = 4.765\n",
      "Epoch  17 Batch    0/450   train_loss = 4.700\n",
      "Epoch  17 Batch  150/450   train_loss = 4.787\n",
      "Epoch  17 Batch  300/450   train_loss = 4.675\n",
      "Epoch  18 Batch    0/450   train_loss = 4.590\n",
      "Epoch  18 Batch  150/450   train_loss = 4.695\n",
      "Epoch  18 Batch  300/450   train_loss = 4.596\n",
      "Epoch  19 Batch    0/450   train_loss = 4.503\n",
      "Epoch  19 Batch  150/450   train_loss = 4.603\n",
      "Epoch  19 Batch  300/450   train_loss = 4.507\n",
      "Epoch  20 Batch    0/450   train_loss = 4.426\n",
      "Epoch  20 Batch  150/450   train_loss = 4.515\n",
      "Epoch  20 Batch  300/450   train_loss = 4.423\n",
      "Epoch  21 Batch    0/450   train_loss = 4.335\n",
      "Epoch  21 Batch  150/450   train_loss = 4.430\n",
      "Epoch  21 Batch  300/450   train_loss = 4.366\n",
      "Epoch  22 Batch    0/450   train_loss = 4.252\n",
      "Epoch  22 Batch  150/450   train_loss = 4.366\n",
      "Epoch  22 Batch  300/450   train_loss = 4.299\n",
      "Epoch  23 Batch    0/450   train_loss = 4.177\n",
      "Epoch  23 Batch  150/450   train_loss = 4.300\n",
      "Epoch  23 Batch  300/450   train_loss = 4.234\n",
      "Epoch  24 Batch    0/450   train_loss = 4.086\n",
      "Epoch  24 Batch  150/450   train_loss = 4.222\n",
      "Epoch  24 Batch  300/450   train_loss = 4.173\n",
      "Epoch  25 Batch    0/450   train_loss = 4.023\n",
      "Epoch  25 Batch  150/450   train_loss = 4.151\n",
      "Epoch  25 Batch  300/450   train_loss = 4.107\n",
      "Epoch  26 Batch    0/450   train_loss = 3.957\n",
      "Epoch  26 Batch  150/450   train_loss = 4.088\n",
      "Epoch  26 Batch  300/450   train_loss = 4.038\n",
      "Epoch  27 Batch    0/450   train_loss = 3.893\n",
      "Epoch  27 Batch  150/450   train_loss = 4.029\n",
      "Epoch  27 Batch  300/450   train_loss = 3.975\n",
      "Epoch  28 Batch    0/450   train_loss = 3.833\n",
      "Epoch  28 Batch  150/450   train_loss = 3.974\n",
      "Epoch  28 Batch  300/450   train_loss = 3.920\n",
      "Epoch  29 Batch    0/450   train_loss = 3.792\n",
      "Epoch  29 Batch  150/450   train_loss = 3.929\n",
      "Epoch  29 Batch  300/450   train_loss = 3.877\n",
      "Epoch  30 Batch    0/450   train_loss = 3.740\n",
      "Epoch  30 Batch  150/450   train_loss = 3.872\n",
      "Epoch  30 Batch  300/450   train_loss = 3.820\n",
      "Epoch  31 Batch    0/450   train_loss = 3.703\n",
      "Epoch  31 Batch  150/450   train_loss = 3.822\n",
      "Epoch  31 Batch  300/450   train_loss = 3.768\n",
      "Epoch  32 Batch    0/450   train_loss = 3.647\n",
      "Epoch  32 Batch  150/450   train_loss = 3.771\n",
      "Epoch  32 Batch  300/450   train_loss = 3.721\n",
      "Epoch  33 Batch    0/450   train_loss = 3.597\n",
      "Epoch  33 Batch  150/450   train_loss = 3.728\n",
      "Epoch  33 Batch  300/450   train_loss = 3.676\n",
      "Epoch  34 Batch    0/450   train_loss = 3.559\n",
      "Epoch  34 Batch  150/450   train_loss = 3.697\n",
      "Epoch  34 Batch  300/450   train_loss = 3.632\n",
      "Epoch  35 Batch    0/450   train_loss = 3.525\n",
      "Epoch  35 Batch  150/450   train_loss = 3.648\n",
      "Epoch  35 Batch  300/450   train_loss = 3.596\n",
      "Epoch  36 Batch    0/450   train_loss = 3.482\n",
      "Epoch  36 Batch  150/450   train_loss = 3.605\n",
      "Epoch  36 Batch  300/450   train_loss = 3.555\n",
      "Epoch  37 Batch    0/450   train_loss = 3.432\n",
      "Epoch  37 Batch  150/450   train_loss = 3.562\n",
      "Epoch  37 Batch  300/450   train_loss = 3.525\n",
      "Epoch  38 Batch    0/450   train_loss = 3.386\n",
      "Epoch  38 Batch  150/450   train_loss = 3.517\n",
      "Epoch  38 Batch  300/450   train_loss = 3.484\n",
      "Epoch  39 Batch    0/450   train_loss = 3.350\n",
      "Epoch  39 Batch  150/450   train_loss = 3.482\n",
      "Epoch  39 Batch  300/450   train_loss = 3.439\n",
      "Epoch  40 Batch    0/450   train_loss = 3.331\n",
      "Epoch  40 Batch  150/450   train_loss = 3.449\n",
      "Epoch  40 Batch  300/450   train_loss = 3.400\n",
      "Epoch  41 Batch    0/450   train_loss = 3.296\n",
      "Epoch  41 Batch  150/450   train_loss = 3.406\n",
      "Epoch  41 Batch  300/450   train_loss = 3.365\n",
      "Epoch  42 Batch    0/450   train_loss = 3.261\n",
      "Epoch  42 Batch  150/450   train_loss = 3.366\n",
      "Epoch  42 Batch  300/450   train_loss = 3.328\n",
      "Epoch  43 Batch    0/450   train_loss = 3.209\n",
      "Epoch  43 Batch  150/450   train_loss = 3.338\n",
      "Epoch  43 Batch  300/450   train_loss = 3.305\n",
      "Epoch  44 Batch    0/450   train_loss = 3.181\n",
      "Epoch  44 Batch  150/450   train_loss = 3.306\n",
      "Epoch  44 Batch  300/450   train_loss = 3.280\n",
      "Epoch  45 Batch    0/450   train_loss = 3.159\n",
      "Epoch  45 Batch  150/450   train_loss = 3.276\n",
      "Epoch  45 Batch  300/450   train_loss = 3.243\n",
      "Epoch  46 Batch    0/450   train_loss = 3.136\n",
      "Epoch  46 Batch  150/450   train_loss = 3.256\n",
      "Epoch  46 Batch  300/450   train_loss = 3.210\n",
      "Epoch  47 Batch    0/450   train_loss = 3.110\n",
      "Epoch  47 Batch  150/450   train_loss = 3.236\n",
      "Epoch  47 Batch  300/450   train_loss = 3.204\n",
      "Epoch  48 Batch    0/450   train_loss = 3.085\n",
      "Epoch  48 Batch  150/450   train_loss = 3.207\n",
      "Epoch  48 Batch  300/450   train_loss = 3.190\n",
      "Epoch  49 Batch    0/450   train_loss = 3.056\n",
      "Epoch  49 Batch  150/450   train_loss = 3.172\n",
      "Epoch  49 Batch  300/450   train_loss = 3.165\n",
      "Epoch  50 Batch    0/450   train_loss = 3.031\n",
      "Epoch  50 Batch  150/450   train_loss = 3.155\n",
      "Epoch  50 Batch  300/450   train_loss = 3.119\n",
      "Epoch  51 Batch    0/450   train_loss = 3.005\n",
      "Epoch  51 Batch  150/450   train_loss = 3.138\n",
      "Epoch  51 Batch  300/450   train_loss = 3.088\n",
      "Epoch  52 Batch    0/450   train_loss = 2.977\n",
      "Epoch  52 Batch  150/450   train_loss = 3.119\n",
      "Epoch  52 Batch  300/450   train_loss = 3.074\n",
      "Epoch  53 Batch    0/450   train_loss = 2.964\n",
      "Epoch  53 Batch  150/450   train_loss = 3.084\n",
      "Epoch  53 Batch  300/450   train_loss = 3.040\n",
      "Epoch  54 Batch    0/450   train_loss = 2.925\n",
      "Epoch  54 Batch  150/450   train_loss = 3.061\n",
      "Epoch  54 Batch  300/450   train_loss = 3.027\n",
      "Epoch  55 Batch    0/450   train_loss = 2.898\n",
      "Epoch  55 Batch  150/450   train_loss = 3.032\n",
      "Epoch  55 Batch  300/450   train_loss = 3.007\n",
      "Epoch  56 Batch    0/450   train_loss = 2.879\n",
      "Epoch  56 Batch  150/450   train_loss = 3.003\n",
      "Epoch  56 Batch  300/450   train_loss = 2.983\n",
      "Epoch  57 Batch    0/450   train_loss = 2.860\n",
      "Epoch  57 Batch  150/450   train_loss = 2.975\n",
      "Epoch  57 Batch  300/450   train_loss = 2.963\n",
      "Epoch  58 Batch    0/450   train_loss = 2.848\n",
      "Epoch  58 Batch  150/450   train_loss = 2.969\n",
      "Epoch  58 Batch  300/450   train_loss = 2.956\n",
      "Epoch  59 Batch    0/450   train_loss = 2.830\n",
      "Epoch  59 Batch  150/450   train_loss = 2.945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  59 Batch  300/450   train_loss = 2.950\n",
      "Epoch  60 Batch    0/450   train_loss = 2.832\n",
      "Epoch  60 Batch  150/450   train_loss = 2.929\n",
      "Epoch  60 Batch  300/450   train_loss = 2.924\n",
      "Epoch  61 Batch    0/450   train_loss = 2.823\n",
      "Epoch  61 Batch  150/450   train_loss = 2.916\n",
      "Epoch  61 Batch  300/450   train_loss = 2.909\n",
      "Epoch  62 Batch    0/450   train_loss = 2.803\n",
      "Epoch  62 Batch  150/450   train_loss = 2.901\n",
      "Epoch  62 Batch  300/450   train_loss = 2.910\n",
      "Epoch  63 Batch    0/450   train_loss = 2.788\n",
      "Epoch  63 Batch  150/450   train_loss = 2.898\n",
      "Epoch  63 Batch  300/450   train_loss = 2.902\n",
      "Epoch  64 Batch    0/450   train_loss = 2.778\n",
      "Epoch  64 Batch  150/450   train_loss = 2.877\n",
      "Epoch  64 Batch  300/450   train_loss = 2.894\n",
      "Epoch  65 Batch    0/450   train_loss = 2.766\n",
      "Epoch  65 Batch  150/450   train_loss = 2.868\n",
      "Epoch  65 Batch  300/450   train_loss = 2.874\n",
      "Epoch  66 Batch    0/450   train_loss = 2.741\n",
      "Epoch  66 Batch  150/450   train_loss = 2.852\n",
      "Epoch  66 Batch  300/450   train_loss = 2.852\n",
      "Epoch  67 Batch    0/450   train_loss = 2.717\n",
      "Epoch  67 Batch  150/450   train_loss = 2.825\n",
      "Epoch  67 Batch  300/450   train_loss = 2.829\n",
      "Epoch  68 Batch    0/450   train_loss = 2.699\n",
      "Epoch  68 Batch  150/450   train_loss = 2.810\n",
      "Epoch  68 Batch  300/450   train_loss = 2.824\n",
      "Epoch  69 Batch    0/450   train_loss = 2.671\n",
      "Epoch  69 Batch  150/450   train_loss = 2.777\n",
      "Epoch  69 Batch  300/450   train_loss = 2.803\n",
      "Epoch  70 Batch    0/450   train_loss = 2.647\n",
      "Epoch  70 Batch  150/450   train_loss = 2.751\n",
      "Epoch  70 Batch  300/450   train_loss = 2.787\n",
      "Epoch  71 Batch    0/450   train_loss = 2.636\n",
      "Epoch  71 Batch  150/450   train_loss = 2.743\n",
      "Epoch  71 Batch  300/450   train_loss = 2.775\n",
      "Epoch  72 Batch    0/450   train_loss = 2.625\n",
      "Epoch  72 Batch  150/450   train_loss = 2.721\n",
      "Epoch  72 Batch  300/450   train_loss = 2.743\n",
      "Epoch  73 Batch    0/450   train_loss = 2.603\n",
      "Epoch  73 Batch  150/450   train_loss = 2.703\n",
      "Epoch  73 Batch  300/450   train_loss = 2.730\n",
      "Epoch  74 Batch    0/450   train_loss = 2.597\n",
      "Epoch  74 Batch  150/450   train_loss = 2.694\n",
      "Epoch  74 Batch  300/450   train_loss = 2.717\n",
      "Epoch  75 Batch    0/450   train_loss = 2.591\n",
      "Epoch  75 Batch  150/450   train_loss = 2.677\n",
      "Epoch  75 Batch  300/450   train_loss = 2.694\n",
      "Epoch  76 Batch    0/450   train_loss = 2.583\n",
      "Epoch  76 Batch  150/450   train_loss = 2.677\n",
      "Epoch  76 Batch  300/450   train_loss = 2.683\n",
      "Epoch  77 Batch    0/450   train_loss = 2.562\n",
      "Epoch  77 Batch  150/450   train_loss = 2.657\n",
      "Epoch  77 Batch  300/450   train_loss = 2.669\n",
      "Epoch  78 Batch    0/450   train_loss = 2.541\n",
      "Epoch  78 Batch  150/450   train_loss = 2.643\n",
      "Epoch  78 Batch  300/450   train_loss = 2.652\n",
      "Epoch  79 Batch    0/450   train_loss = 2.521\n",
      "Epoch  79 Batch  150/450   train_loss = 2.634\n",
      "Epoch  79 Batch  300/450   train_loss = 2.636\n",
      "Epoch  80 Batch    0/450   train_loss = 2.524\n",
      "Epoch  80 Batch  150/450   train_loss = 2.629\n",
      "Epoch  80 Batch  300/450   train_loss = 2.632\n",
      "Epoch  81 Batch    0/450   train_loss = 2.535\n",
      "Epoch  81 Batch  150/450   train_loss = 2.614\n",
      "Epoch  81 Batch  300/450   train_loss = 2.634\n",
      "Epoch  82 Batch    0/450   train_loss = 2.528\n",
      "Epoch  82 Batch  150/450   train_loss = 2.605\n",
      "Epoch  82 Batch  300/450   train_loss = 2.631\n",
      "Epoch  83 Batch    0/450   train_loss = 2.508\n",
      "Epoch  83 Batch  150/450   train_loss = 2.598\n",
      "Epoch  83 Batch  300/450   train_loss = 2.635\n",
      "Epoch  84 Batch    0/450   train_loss = 2.499\n",
      "Epoch  84 Batch  150/450   train_loss = 2.595\n",
      "Epoch  84 Batch  300/450   train_loss = 2.616\n",
      "Epoch  85 Batch    0/450   train_loss = 2.472\n",
      "Epoch  85 Batch  150/450   train_loss = 2.579\n",
      "Epoch  85 Batch  300/450   train_loss = 2.599\n",
      "Epoch  86 Batch    0/450   train_loss = 2.472\n",
      "Epoch  86 Batch  150/450   train_loss = 2.567\n",
      "Epoch  86 Batch  300/450   train_loss = 2.587\n",
      "Epoch  87 Batch    0/450   train_loss = 2.466\n",
      "Epoch  87 Batch  150/450   train_loss = 2.561\n",
      "Epoch  87 Batch  300/450   train_loss = 2.569\n",
      "Epoch  88 Batch    0/450   train_loss = 2.449\n",
      "Epoch  88 Batch  150/450   train_loss = 2.549\n",
      "Epoch  88 Batch  300/450   train_loss = 2.565\n",
      "Epoch  89 Batch    0/450   train_loss = 2.435\n",
      "Epoch  89 Batch  150/450   train_loss = 2.534\n",
      "Epoch  89 Batch  300/450   train_loss = 2.556\n",
      "Epoch  90 Batch    0/450   train_loss = 2.421\n",
      "Epoch  90 Batch  150/450   train_loss = 2.525\n",
      "Epoch  90 Batch  300/450   train_loss = 2.530\n",
      "Epoch  91 Batch    0/450   train_loss = 2.417\n",
      "Epoch  91 Batch  150/450   train_loss = 2.502\n",
      "Epoch  91 Batch  300/450   train_loss = 2.533\n",
      "Epoch  92 Batch    0/450   train_loss = 2.406\n",
      "Epoch  92 Batch  150/450   train_loss = 2.499\n",
      "Epoch  92 Batch  300/450   train_loss = 2.517\n",
      "Epoch  93 Batch    0/450   train_loss = 2.393\n",
      "Epoch  93 Batch  150/450   train_loss = 2.497\n",
      "Epoch  93 Batch  300/450   train_loss = 2.500\n",
      "Epoch  94 Batch    0/450   train_loss = 2.389\n",
      "Epoch  94 Batch  150/450   train_loss = 2.480\n",
      "Epoch  94 Batch  300/450   train_loss = 2.491\n",
      "Epoch  95 Batch    0/450   train_loss = 2.394\n",
      "Epoch  95 Batch  150/450   train_loss = 2.486\n",
      "Epoch  95 Batch  300/450   train_loss = 2.475\n",
      "Epoch  96 Batch    0/450   train_loss = 2.375\n",
      "Epoch  96 Batch  150/450   train_loss = 2.475\n",
      "Epoch  96 Batch  300/450   train_loss = 2.483\n",
      "Epoch  97 Batch    0/450   train_loss = 2.363\n",
      "Epoch  97 Batch  150/450   train_loss = 2.464\n",
      "Epoch  97 Batch  300/450   train_loss = 2.455\n",
      "Epoch  98 Batch    0/450   train_loss = 2.361\n",
      "Epoch  98 Batch  150/450   train_loss = 2.462\n",
      "Epoch  98 Batch  300/450   train_loss = 2.456\n",
      "Epoch  99 Batch    0/450   train_loss = 2.345\n",
      "Epoch  99 Batch  150/450   train_loss = 2.444\n",
      "Epoch  99 Batch  300/450   train_loss = 2.436\n",
      "Epoch 100 Batch    0/450   train_loss = 2.329\n",
      "Epoch 100 Batch  150/450   train_loss = 2.438\n",
      "Epoch 100 Batch  300/450   train_loss = 2.433\n",
      "Epoch 101 Batch    0/450   train_loss = 2.324\n",
      "Epoch 101 Batch  150/450   train_loss = 2.426\n",
      "Epoch 101 Batch  300/450   train_loss = 2.429\n",
      "Epoch 102 Batch    0/450   train_loss = 2.329\n",
      "Epoch 102 Batch  150/450   train_loss = 2.428\n",
      "Epoch 102 Batch  300/450   train_loss = 2.415\n",
      "Epoch 103 Batch    0/450   train_loss = 2.316\n",
      "Epoch 103 Batch  150/450   train_loss = 2.419\n",
      "Epoch 103 Batch  300/450   train_loss = 2.408\n",
      "Epoch 104 Batch    0/450   train_loss = 2.294\n",
      "Epoch 104 Batch  150/450   train_loss = 2.407\n",
      "Epoch 104 Batch  300/450   train_loss = 2.403\n",
      "Epoch 105 Batch    0/450   train_loss = 2.284\n",
      "Epoch 105 Batch  150/450   train_loss = 2.404\n",
      "Epoch 105 Batch  300/450   train_loss = 2.406\n",
      "Epoch 106 Batch    0/450   train_loss = 2.279\n",
      "Epoch 106 Batch  150/450   train_loss = 2.392\n",
      "Epoch 106 Batch  300/450   train_loss = 2.389\n",
      "Epoch 107 Batch    0/450   train_loss = 2.267\n",
      "Epoch 107 Batch  150/450   train_loss = 2.397\n",
      "Epoch 107 Batch  300/450   train_loss = 2.378\n",
      "Epoch 108 Batch    0/450   train_loss = 2.255\n",
      "Epoch 108 Batch  150/450   train_loss = 2.388\n",
      "Epoch 108 Batch  300/450   train_loss = 2.369\n",
      "Epoch 109 Batch    0/450   train_loss = 2.261\n",
      "Epoch 109 Batch  150/450   train_loss = 2.382\n",
      "Epoch 109 Batch  300/450   train_loss = 2.358\n",
      "Epoch 110 Batch    0/450   train_loss = 2.246\n",
      "Epoch 110 Batch  150/450   train_loss = 2.371\n",
      "Epoch 110 Batch  300/450   train_loss = 2.338\n",
      "Epoch 111 Batch    0/450   train_loss = 2.252\n",
      "Epoch 111 Batch  150/450   train_loss = 2.371\n",
      "Epoch 111 Batch  300/450   train_loss = 2.345\n",
      "Epoch 112 Batch    0/450   train_loss = 2.227\n",
      "Epoch 112 Batch  150/450   train_loss = 2.352\n",
      "Epoch 112 Batch  300/450   train_loss = 2.334\n",
      "Epoch 113 Batch    0/450   train_loss = 2.221\n",
      "Epoch 113 Batch  150/450   train_loss = 2.348\n",
      "Epoch 113 Batch  300/450   train_loss = 2.314\n",
      "Epoch 114 Batch    0/450   train_loss = 2.242\n",
      "Epoch 114 Batch  150/450   train_loss = 2.339\n",
      "Epoch 114 Batch  300/450   train_loss = 2.307\n",
      "Epoch 115 Batch    0/450   train_loss = 2.218\n",
      "Epoch 115 Batch  150/450   train_loss = 2.329\n",
      "Epoch 115 Batch  300/450   train_loss = 2.299\n",
      "Epoch 116 Batch    0/450   train_loss = 2.219\n",
      "Epoch 116 Batch  150/450   train_loss = 2.325\n",
      "Epoch 116 Batch  300/450   train_loss = 2.287\n",
      "Epoch 117 Batch    0/450   train_loss = 2.215\n",
      "Epoch 117 Batch  150/450   train_loss = 2.322\n",
      "Epoch 117 Batch  300/450   train_loss = 2.296\n",
      "Epoch 118 Batch    0/450   train_loss = 2.199\n",
      "Epoch 118 Batch  150/450   train_loss = 2.306\n",
      "Epoch 118 Batch  300/450   train_loss = 2.296\n",
      "Epoch 119 Batch    0/450   train_loss = 2.212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119 Batch  150/450   train_loss = 2.292\n",
      "Epoch 119 Batch  300/450   train_loss = 2.287\n",
      "Epoch 120 Batch    0/450   train_loss = 2.202\n",
      "Epoch 120 Batch  150/450   train_loss = 2.288\n",
      "Epoch 120 Batch  300/450   train_loss = 2.286\n",
      "Epoch 121 Batch    0/450   train_loss = 2.201\n",
      "Epoch 121 Batch  150/450   train_loss = 2.286\n",
      "Epoch 121 Batch  300/450   train_loss = 2.263\n",
      "Epoch 122 Batch    0/450   train_loss = 2.188\n",
      "Epoch 122 Batch  150/450   train_loss = 2.282\n",
      "Epoch 122 Batch  300/450   train_loss = 2.254\n",
      "Epoch 123 Batch    0/450   train_loss = 2.175\n",
      "Epoch 123 Batch  150/450   train_loss = 2.281\n",
      "Epoch 123 Batch  300/450   train_loss = 2.253\n",
      "Epoch 124 Batch    0/450   train_loss = 2.172\n",
      "Epoch 124 Batch  150/450   train_loss = 2.273\n",
      "Epoch 124 Batch  300/450   train_loss = 2.249\n",
      "Epoch 125 Batch    0/450   train_loss = 2.184\n",
      "Epoch 125 Batch  150/450   train_loss = 2.283\n",
      "Epoch 125 Batch  300/450   train_loss = 2.245\n",
      "Epoch 126 Batch    0/450   train_loss = 2.160\n",
      "Epoch 126 Batch  150/450   train_loss = 2.263\n",
      "Epoch 126 Batch  300/450   train_loss = 2.239\n",
      "Epoch 127 Batch    0/450   train_loss = 2.152\n",
      "Epoch 127 Batch  150/450   train_loss = 2.260\n",
      "Epoch 127 Batch  300/450   train_loss = 2.241\n",
      "Epoch 128 Batch    0/450   train_loss = 2.151\n",
      "Epoch 128 Batch  150/450   train_loss = 2.254\n",
      "Epoch 128 Batch  300/450   train_loss = 2.236\n",
      "Epoch 129 Batch    0/450   train_loss = 2.147\n",
      "Epoch 129 Batch  150/450   train_loss = 2.247\n",
      "Epoch 129 Batch  300/450   train_loss = 2.218\n",
      "Epoch 130 Batch    0/450   train_loss = 2.121\n",
      "Epoch 130 Batch  150/450   train_loss = 2.231\n",
      "Epoch 130 Batch  300/450   train_loss = 2.202\n",
      "Epoch 131 Batch    0/450   train_loss = 2.119\n",
      "Epoch 131 Batch  150/450   train_loss = 2.227\n",
      "Epoch 131 Batch  300/450   train_loss = 2.209\n",
      "Epoch 132 Batch    0/450   train_loss = 2.111\n",
      "Epoch 132 Batch  150/450   train_loss = 2.215\n",
      "Epoch 132 Batch  300/450   train_loss = 2.204\n",
      "Epoch 133 Batch    0/450   train_loss = 2.084\n",
      "Epoch 133 Batch  150/450   train_loss = 2.212\n",
      "Epoch 133 Batch  300/450   train_loss = 2.186\n",
      "Epoch 134 Batch    0/450   train_loss = 2.095\n",
      "Epoch 134 Batch  150/450   train_loss = 2.198\n",
      "Epoch 134 Batch  300/450   train_loss = 2.188\n",
      "Epoch 135 Batch    0/450   train_loss = 2.068\n",
      "Epoch 135 Batch  150/450   train_loss = 2.199\n",
      "Epoch 135 Batch  300/450   train_loss = 2.176\n",
      "Epoch 136 Batch    0/450   train_loss = 2.077\n",
      "Epoch 136 Batch  150/450   train_loss = 2.187\n",
      "Epoch 136 Batch  300/450   train_loss = 2.181\n",
      "Epoch 137 Batch    0/450   train_loss = 2.080\n",
      "Epoch 137 Batch  150/450   train_loss = 2.185\n",
      "Epoch 137 Batch  300/450   train_loss = 2.177\n",
      "Epoch 138 Batch    0/450   train_loss = 2.078\n",
      "Epoch 138 Batch  150/450   train_loss = 2.174\n",
      "Epoch 138 Batch  300/450   train_loss = 2.171\n",
      "Epoch 139 Batch    0/450   train_loss = 2.063\n",
      "Epoch 139 Batch  150/450   train_loss = 2.179\n",
      "Epoch 139 Batch  300/450   train_loss = 2.162\n",
      "Epoch 140 Batch    0/450   train_loss = 2.082\n",
      "Epoch 140 Batch  150/450   train_loss = 2.177\n",
      "Epoch 140 Batch  300/450   train_loss = 2.153\n",
      "Epoch 141 Batch    0/450   train_loss = 2.076\n",
      "Epoch 141 Batch  150/450   train_loss = 2.171\n",
      "Epoch 141 Batch  300/450   train_loss = 2.163\n",
      "Epoch 142 Batch    0/450   train_loss = 2.076\n",
      "Epoch 142 Batch  150/450   train_loss = 2.173\n",
      "Epoch 142 Batch  300/450   train_loss = 2.154\n",
      "Epoch 143 Batch    0/450   train_loss = 2.056\n",
      "Epoch 143 Batch  150/450   train_loss = 2.160\n",
      "Epoch 143 Batch  300/450   train_loss = 2.143\n",
      "Epoch 144 Batch    0/450   train_loss = 2.046\n",
      "Epoch 144 Batch  150/450   train_loss = 2.140\n",
      "Epoch 144 Batch  300/450   train_loss = 2.144\n",
      "Epoch 145 Batch    0/450   train_loss = 2.034\n",
      "Epoch 145 Batch  150/450   train_loss = 2.150\n",
      "Epoch 145 Batch  300/450   train_loss = 2.132\n",
      "Epoch 146 Batch    0/450   train_loss = 2.032\n",
      "Epoch 146 Batch  150/450   train_loss = 2.132\n",
      "Epoch 146 Batch  300/450   train_loss = 2.136\n",
      "Epoch 147 Batch    0/450   train_loss = 2.029\n",
      "Epoch 147 Batch  150/450   train_loss = 2.132\n",
      "Epoch 147 Batch  300/450   train_loss = 2.124\n",
      "Epoch 148 Batch    0/450   train_loss = 2.030\n",
      "Epoch 148 Batch  150/450   train_loss = 2.129\n",
      "Epoch 148 Batch  300/450   train_loss = 2.126\n",
      "Epoch 149 Batch    0/450   train_loss = 2.037\n",
      "Epoch 149 Batch  150/450   train_loss = 2.137\n",
      "Epoch 149 Batch  300/450   train_loss = 2.120\n",
      "Epoch 150 Batch    0/450   train_loss = 2.026\n",
      "Epoch 150 Batch  150/450   train_loss = 2.134\n",
      "Epoch 150 Batch  300/450   train_loss = 2.108\n",
      "Epoch 151 Batch    0/450   train_loss = 2.029\n",
      "Epoch 151 Batch  150/450   train_loss = 2.120\n",
      "Epoch 151 Batch  300/450   train_loss = 2.108\n",
      "Epoch 152 Batch    0/450   train_loss = 2.012\n",
      "Epoch 152 Batch  150/450   train_loss = 2.107\n",
      "Epoch 152 Batch  300/450   train_loss = 2.111\n",
      "Epoch 153 Batch    0/450   train_loss = 2.015\n",
      "Epoch 153 Batch  150/450   train_loss = 2.102\n",
      "Epoch 153 Batch  300/450   train_loss = 2.107\n",
      "Epoch 154 Batch    0/450   train_loss = 2.024\n",
      "Epoch 154 Batch  150/450   train_loss = 2.093\n",
      "Epoch 154 Batch  300/450   train_loss = 2.090\n",
      "Epoch 155 Batch    0/450   train_loss = 1.996\n",
      "Epoch 155 Batch  150/450   train_loss = 2.097\n",
      "Epoch 155 Batch  300/450   train_loss = 2.099\n",
      "Epoch 156 Batch    0/450   train_loss = 1.995\n",
      "Epoch 156 Batch  150/450   train_loss = 2.091\n",
      "Epoch 156 Batch  300/450   train_loss = 2.101\n",
      "Epoch 157 Batch    0/450   train_loss = 2.005\n",
      "Epoch 157 Batch  150/450   train_loss = 2.073\n",
      "Epoch 157 Batch  300/450   train_loss = 2.090\n",
      "Epoch 158 Batch    0/450   train_loss = 1.986\n",
      "Epoch 158 Batch  150/450   train_loss = 2.067\n",
      "Epoch 158 Batch  300/450   train_loss = 2.097\n",
      "Epoch 159 Batch    0/450   train_loss = 1.998\n",
      "Epoch 159 Batch  150/450   train_loss = 2.071\n",
      "Epoch 159 Batch  300/450   train_loss = 2.079\n",
      "Epoch 160 Batch    0/450   train_loss = 1.973\n",
      "Epoch 160 Batch  150/450   train_loss = 2.067\n",
      "Epoch 160 Batch  300/450   train_loss = 2.074\n",
      "Epoch 161 Batch    0/450   train_loss = 1.969\n",
      "Epoch 161 Batch  150/450   train_loss = 2.052\n",
      "Epoch 161 Batch  300/450   train_loss = 2.069\n",
      "Epoch 162 Batch    0/450   train_loss = 1.962\n",
      "Epoch 162 Batch  150/450   train_loss = 2.052\n",
      "Epoch 162 Batch  300/450   train_loss = 2.054\n",
      "Epoch 163 Batch    0/450   train_loss = 1.962\n",
      "Epoch 163 Batch  150/450   train_loss = 2.052\n",
      "Epoch 163 Batch  300/450   train_loss = 2.060\n",
      "Epoch 164 Batch    0/450   train_loss = 1.954\n",
      "Epoch 164 Batch  150/450   train_loss = 2.057\n",
      "Epoch 164 Batch  300/450   train_loss = 2.058\n",
      "Epoch 165 Batch    0/450   train_loss = 1.947\n",
      "Epoch 165 Batch  150/450   train_loss = 2.042\n",
      "Epoch 165 Batch  300/450   train_loss = 2.052\n",
      "Epoch 166 Batch    0/450   train_loss = 1.942\n",
      "Epoch 166 Batch  150/450   train_loss = 2.037\n",
      "Epoch 166 Batch  300/450   train_loss = 2.049\n",
      "Epoch 167 Batch    0/450   train_loss = 1.941\n",
      "Epoch 167 Batch  150/450   train_loss = 2.046\n",
      "Epoch 167 Batch  300/450   train_loss = 2.047\n",
      "Epoch 168 Batch    0/450   train_loss = 1.921\n",
      "Epoch 168 Batch  150/450   train_loss = 2.036\n",
      "Epoch 168 Batch  300/450   train_loss = 2.038\n",
      "Epoch 169 Batch    0/450   train_loss = 1.925\n",
      "Epoch 169 Batch  150/450   train_loss = 2.032\n",
      "Epoch 169 Batch  300/450   train_loss = 2.037\n",
      "Epoch 170 Batch    0/450   train_loss = 1.929\n",
      "Epoch 170 Batch  150/450   train_loss = 2.027\n",
      "Epoch 170 Batch  300/450   train_loss = 2.018\n",
      "Epoch 171 Batch    0/450   train_loss = 1.909\n",
      "Epoch 171 Batch  150/450   train_loss = 2.015\n",
      "Epoch 171 Batch  300/450   train_loss = 2.034\n",
      "Epoch 172 Batch    0/450   train_loss = 1.939\n",
      "Epoch 172 Batch  150/450   train_loss = 2.032\n",
      "Epoch 172 Batch  300/450   train_loss = 2.036\n",
      "Epoch 173 Batch    0/450   train_loss = 1.926\n",
      "Epoch 173 Batch  150/450   train_loss = 2.012\n",
      "Epoch 173 Batch  300/450   train_loss = 2.021\n",
      "Epoch 174 Batch    0/450   train_loss = 1.905\n",
      "Epoch 174 Batch  150/450   train_loss = 1.996\n",
      "Epoch 174 Batch  300/450   train_loss = 2.010\n",
      "Epoch 175 Batch    0/450   train_loss = 1.911\n",
      "Epoch 175 Batch  150/450   train_loss = 2.007\n",
      "Epoch 175 Batch  300/450   train_loss = 2.010\n",
      "Epoch 176 Batch    0/450   train_loss = 1.908\n",
      "Epoch 176 Batch  150/450   train_loss = 1.993\n",
      "Epoch 176 Batch  300/450   train_loss = 2.005\n",
      "Epoch 177 Batch    0/450   train_loss = 1.908\n",
      "Epoch 177 Batch  150/450   train_loss = 1.992\n",
      "Epoch 177 Batch  300/450   train_loss = 2.016\n",
      "Epoch 178 Batch    0/450   train_loss = 1.918\n",
      "Epoch 178 Batch  150/450   train_loss = 1.990\n",
      "Epoch 178 Batch  300/450   train_loss = 2.006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179 Batch    0/450   train_loss = 1.908\n",
      "Epoch 179 Batch  150/450   train_loss = 1.997\n",
      "Epoch 179 Batch  300/450   train_loss = 1.997\n",
      "Epoch 180 Batch    0/450   train_loss = 1.893\n",
      "Epoch 180 Batch  150/450   train_loss = 1.979\n",
      "Epoch 180 Batch  300/450   train_loss = 2.001\n",
      "Epoch 181 Batch    0/450   train_loss = 1.895\n",
      "Epoch 181 Batch  150/450   train_loss = 1.978\n",
      "Epoch 181 Batch  300/450   train_loss = 1.996\n",
      "Epoch 182 Batch    0/450   train_loss = 1.883\n",
      "Epoch 182 Batch  150/450   train_loss = 1.983\n",
      "Epoch 182 Batch  300/450   train_loss = 1.993\n",
      "Epoch 183 Batch    0/450   train_loss = 1.892\n",
      "Epoch 183 Batch  150/450   train_loss = 1.978\n",
      "Epoch 183 Batch  300/450   train_loss = 2.000\n",
      "Epoch 184 Batch    0/450   train_loss = 1.889\n",
      "Epoch 184 Batch  150/450   train_loss = 1.958\n",
      "Epoch 184 Batch  300/450   train_loss = 1.990\n",
      "Epoch 185 Batch    0/450   train_loss = 1.878\n",
      "Epoch 185 Batch  150/450   train_loss = 1.962\n",
      "Epoch 185 Batch  300/450   train_loss = 1.986\n",
      "Epoch 186 Batch    0/450   train_loss = 1.894\n",
      "Epoch 186 Batch  150/450   train_loss = 1.964\n",
      "Epoch 186 Batch  300/450   train_loss = 1.994\n",
      "Epoch 187 Batch    0/450   train_loss = 1.893\n",
      "Epoch 187 Batch  150/450   train_loss = 1.945\n",
      "Epoch 187 Batch  300/450   train_loss = 1.979\n",
      "Epoch 188 Batch    0/450   train_loss = 1.873\n",
      "Epoch 188 Batch  150/450   train_loss = 1.947\n",
      "Epoch 188 Batch  300/450   train_loss = 1.972\n",
      "Epoch 189 Batch    0/450   train_loss = 1.872\n",
      "Epoch 189 Batch  150/450   train_loss = 1.957\n",
      "Epoch 189 Batch  300/450   train_loss = 1.969\n",
      "Epoch 190 Batch    0/450   train_loss = 1.856\n",
      "Epoch 190 Batch  150/450   train_loss = 1.954\n",
      "Epoch 190 Batch  300/450   train_loss = 1.967\n",
      "Epoch 191 Batch    0/450   train_loss = 1.858\n",
      "Epoch 191 Batch  150/450   train_loss = 1.955\n",
      "Epoch 191 Batch  300/450   train_loss = 1.972\n",
      "Epoch 192 Batch    0/450   train_loss = 1.866\n",
      "Epoch 192 Batch  150/450   train_loss = 1.942\n",
      "Epoch 192 Batch  300/450   train_loss = 1.955\n",
      "Epoch 193 Batch    0/450   train_loss = 1.847\n",
      "Epoch 193 Batch  150/450   train_loss = 1.941\n",
      "Epoch 193 Batch  300/450   train_loss = 1.948\n",
      "Epoch 194 Batch    0/450   train_loss = 1.856\n",
      "Epoch 194 Batch  150/450   train_loss = 1.932\n",
      "Epoch 194 Batch  300/450   train_loss = 1.973\n",
      "Epoch 195 Batch    0/450   train_loss = 1.852\n",
      "Epoch 195 Batch  150/450   train_loss = 1.936\n",
      "Epoch 195 Batch  300/450   train_loss = 1.962\n",
      "Epoch 196 Batch    0/450   train_loss = 1.843\n",
      "Epoch 196 Batch  150/450   train_loss = 1.919\n",
      "Epoch 196 Batch  300/450   train_loss = 1.953\n",
      "Epoch 197 Batch    0/450   train_loss = 1.843\n",
      "Epoch 197 Batch  150/450   train_loss = 1.932\n",
      "Epoch 197 Batch  300/450   train_loss = 1.949\n",
      "Epoch 198 Batch    0/450   train_loss = 1.832\n",
      "Epoch 198 Batch  150/450   train_loss = 1.932\n",
      "Epoch 198 Batch  300/450   train_loss = 1.948\n",
      "Epoch 199 Batch    0/450   train_loss = 1.834\n",
      "Epoch 199 Batch  150/450   train_loss = 1.924\n",
      "Epoch 199 Batch  300/450   train_loss = 1.942\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some helper functions to generate the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get tensors state from saved model\n",
    "\n",
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    inputs = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    return inputs, initial_state, final_state, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random probability for choose words\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "\n",
    "    choices = np.random.choice(len(int_to_vocab), size=1, p=probabilities)\n",
    "    choice  = choices[0]\n",
    "    return int_to_vocab[choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the script\n",
    "\n",
    "Let's generate our script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "Homer Simpson: (HUMS, THEN:) Oh. (Hallway: INT. BUS Patty Bouvier: Yes, if books be careful now, Homer. I thought we were following out. Lenny Leonard: Can I tell anyone! Homer Simpson: (SWEETLY) Yeah. I'm gonna play guitar. Lisa Simpson: That's it! What are you so happy about? Mary: Lisa, wanna any stock, left on a sea issue. We hurt running against him down. Homer Simpson: (SIGHS) Great. I got your job to sting the snail than (SNEERING) (Suburban Street: EXT. country goes - LATER) Apu Nahasapeemapetilon: There for me twenty-four team. (COUGHS OFF Normally I received our community and a line. (Highway: Ext. highway - continuous) (Simpson Home: int. Simpson house - living room - continuous) Edna Krabappel-Flanders: (INTO cab on there?! I see the whole deal. All anyone who is offering for it, Mom. Marge Simpson: You got it, Marge, what's it. Marge Simpson: Well, that brought that thought. Homer Simpson: Gunderson! I can't waste a good house so I can be happy? Connie: It's piss it out. Ned Flanders: Oh, okay. Waylon Smithers: Well... I guess he should have been in the city. Homer Simpson: I'll save that baby I've read it. (Springfield Wax Museum: ext. capital seas it onstage of Congratulations, pray we got away my tear with Springfield tree! Mary: (DISGUSTED SOUND) My finger student, Mr. Smithers. Back in the San pie! (SINGS) ON THE-- Int. house & (Sideshow Player: Hey look! I still haven't sleeping this locked down. Marge Simpson: No more important trouble. Darcy: ... the cameraman! (Springfield Elementary School: INT. springfield elementary - skinner's office - later that night) Committee #2: We've heard your life for someone who... Edna Krabappel-Flanders: (TO QUIMBY) Do it when not. As as the law from bachelor tonight, by Troy McClure. You don't have to be out there and\n"
     ]
    }
   ],
   "source": [
    "gen_length = 300\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'Homer'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word ]#+ ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "    #tv_script = re.findall(r'^([^:-][^:]*):', '\\n', tv_script)\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting generated script\n",
    "\n",
    "Homer Simpson: (HUMS, THEN:) Oh. \n",
    "\n",
    "(Hallway: INT. BUS)\n",
    "\n",
    "Patty Bouvier: Yes, if books be careful now, Homer. I thought we were following out.<br>\n",
    "Lenny Leonard: Can I tell anyone!<br>\n",
    "Homer Simpson: (SWEETLY) Yeah. I'm gonna play guitar. <br>\n",
    "Lisa Simpson: That's it! What are you so happy about? <br>\n",
    "Mary: Lisa, wanna any stock, left on a sea issue. We hurt running against him down. <br>\n",
    "Homer Simpson: (SIGHS) Great. I got your job to sting the snail than (SNEERING)\n",
    "\n",
    "(Suburban Street: EXT. country goes - LATER) \n",
    "\n",
    "Apu Nahasapeemapetilon: There for me twenty-four team. (COUGHS OFF) Normally I received our community and a line. \n",
    "\n",
    "(Highway: Ext. highway - continuous)\n",
    "(Simpson Home: int. Simpson house - living room - continuous)\n",
    "\n",
    "Edna Krabappel-Flanders: (INTO cab on there?! I see the whole deal. All anyone who is offering for it, Mom. <br>\n",
    "Marge Simpson: You got it, Marge, what's it. <br>\n",
    "Marge Simpson: Well, that brought that thought. <br>\n",
    "Homer Simpson: Gunderson! I can't waste a good house so I can be happy? <br>\n",
    "Connie: It's piss it out. <br>\n",
    "Ned Flanders: Oh, okay. <br>\n",
    "Waylon Smithers: Well... I guess he should have been in the city. <br>\n",
    "Homer Simpson: I'll save that baby I've read it. <br>\n",
    "\n",
    "(Springfield Wax Museum: ext. capital seas it onstage of Congratulations, pray we got away my tear with Springfield tree!\n",
    "\n",
    "Mary: (DISGUSTED SOUND) My finger student, Mr. Smithers. Back in the San pie! (SINGS) ON THE-- Int. house & (Sideshow <br> Player: Hey look! I still haven't sleeping this locked down. <br>\n",
    "Marge Simpson: No more important trouble. <br>\n",
    "Darcy: ... the cameraman! \n",
    "\n",
    "(Springfield Elementary School: INT. springfield elementary - skinner's office - later that night)\n",
    "\n",
    "Committee #2: We've heard your life for someone who... <br>\n",
    "Edna Krabappel-Flanders: (TO QUIMBY) Do it when not. As as the law from bachelor tonight, by Troy McClure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The script does not make sense, but we could generate some lines with almost perfect sense! This is an amazing result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
